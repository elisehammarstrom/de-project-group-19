{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad3bfcb-d64b-4a49-ba26-6a97234d0c29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 07:50:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://de-i-19:7077\")\\\n",
    "        .appName(\"Project_Group19\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"300s\")\\\n",
    "        .config(\"spark.executor.cores\",1)\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", \"false\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "# spark_context.setLogLevel(\"INFO\")\n",
    "spark_context.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c15983e-72c0-46ad-be4b-44ae09cb608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf81cc41-d266-4e97-81a6-54b5d478cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lines(lines):\n",
    "    tokenized_lines = lines.map(lambda line: line.split('\"'))\n",
    "    return tokenized_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec09eb0b-a41a-4d22-8cd3-4e1a6fc0dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tabular_data(lines):\n",
    "    tabular_data = []\n",
    "\n",
    "    for line in lines :\n",
    "        csv_line = StringIO(line[0].replace('\\t'))\n",
    "        reader = csv.reader(csv_line, delimiter=',')\n",
    "        tabular_data.extend(reader)\n",
    "\n",
    "    # Convert the tabular data to RDD\n",
    "    rdd = spark_session.sparkContext.parallelize(tabular_data)\n",
    "\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cb6df7-ccb7-4e96-96ba-c78519f9d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(lines):\n",
    "    lowercased_lines = lines.map(lambda line: line.lower())\n",
    "    tokenized_lines = tokenize_lines(lowercased_lines)\n",
    "    return tokenized_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7751032-2169-4c8b-8395-8b699404a475",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marketplace\\tcustomer_id\\treview_id\\tproduct_id\\tproduct_parent\\tproduct_title\\tproduct_category\\tstar_rating\\thelpful_votes\\ttotal_votes\\tvine\\tverified_purchase\\treview_headline\\treview_body\\treview_date', 'US\\t9970739\\tR8EWA1OFT84NX\\tB00GSP5D94\\t329991347\\tSummer Infant SwaddleMe Adjustable Infant Wrap, 3 Count\\tBaby\\t5\\t0\\t0\\tN\\tY\\tGreat swaddled blankets\\tLoved these swaddle blankets and so did my daughter, she slept so much better when tightly swaddled. Only problem was we had a very hard time getting her to sleep without it when she started rolling.\\t2015-08-31', 'US\\t23538442\\tR2JWY4YRQD4FOP\\tB00YYDDZGU\\t646108902\\tPacifier Clip Girl (3 Pack) Ziggy Baby 2-Sided Design Pacifier Holder\\tBaby\\t5\\t0\\t0\\tN\\tN\\tToo cute and really nice\\tThese are adorable pacifier clips. SavvyBaby has elevated the Pacifier Holder to a new level. These are double sided strips of cotton with a loop on one end and a solid clasp on the other.<br /><br />They can be used for not only keep pacifiers off the floor but also for toys, bibs, teethers, and other small items. We looped two together to use as a bib holder while dining out..perfect with the cloth napkin from the restaurant.<br /><br />The pink and brown designs are just to pretty. They are perfect for little girls.<br /><br />I like that the pack includes three clips. It really makes this a great value purchase.<br /><br />Promotional or discounted product provided for review.\\t2015-08-31', 'US\\t8273344\\tRL5ESX231LZ0B\\tB00BUBNZC8\\t642922361\\tUdder Covers - Breast Feeding Nursing Cover\\tBaby\\t5\\t0\\t0\\tN\\tY\\tFive Stars\\tGreat gift\\t2015-08-31', \"US\\t24557753\\tRRMS9ZWJ2KD08\\tB00AWLZFTS\\t494272733\\tGerber Graduates Fun Pack Utensils\\tBaby\\t5\\t0\\t0\\tN\\tY\\tCute; wash up nicely in dishwasher.\\tThese forks are great for my 10 month old daughter! Wanted to find a nice plastic fork for her and I couldn't find a set without spoons but I really like the spoons as well. Nice size for small mouths.\\t2015-08-31\", 'US\\t46263340\\tR14I3ZG5E6S7YM\\tB00KM60D3Q\\t305813185\\tSummer Infant Ultra Sight Pan/Scan/Zoom Video Baby Monitor\\tBaby\\t5\\t0\\t0\\tN\\tY\\tLove it!\\tI wanted something for piece of mind with my little one, but I wanted to stay under $150. This monitor suits our needs, and the price was great.\\t2015-08-31', 'US\\t24557753\\tR13EPSFP5DODN5\\tB00PQMRZG4\\t607341708\\tSummer Infant Keep Me Clean Disposable Bibs, 20-Count\\tBaby\\t4\\t0\\t0\\tN\\tY\\tRips easily.\\tLoved with daughter about 4-6 months. After that, child will realize they are easily ripped off and the bib is useless.\\t2015-08-31', \"US\\t33520065\\tR6RBP4HTE67SY\\tB005DL5970\\t971881542\\tNatural HE Powder Laundry Detergent for Hard Water by Rockin' Green, Perfect for Cloth Diapers, Hard Rock Formula for Hard Water, Up to 90 Loads Per Bag, 45 oz.\\tBaby\\t5\\t0\\t0\\tN\\tY\\tCloth Diaper Detergent\\tThis is a great detergent for cloth diapers!!!\\t2015-08-31\", \"US\\t20241560\\tR15B3EU40RSU2W\\tB00C6D2WL4\\t93827401\\tDr. Brown's Bottle Brush, Pink\\tBaby\\t5\\t0\\t0\\tN\\tY\\tFive Stars\\tgood\\t2015-08-31\", \"US\\t9987983\\tRP4DD53A4ZJA2\\tB0083973FK\\t958629336\\tSposie Booster Pads Diaper Doubler, 90 Count, 3 Packs of 30 Pads\\tBaby\\t5\\t0\\t0\\tN\\tY\\tFive Stars\\tWith these my milk/water loving toddler doesn't wet himself at night.\\t2015-08-31\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark_context.textFile(\"hdfs://192.168.2.246:9000/user/ubuntu/amazon_reviews_us_Baby_v1_00.tsv\")\n",
    "print(data.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa1b2aa1-1382-437a-9b4e-26b92270e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "tokenized_lines = preprocess_data(data)\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aff8b8b-39d3-4c81-9324-244074247c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = tokenized_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33d8f0b5-2838-4b9e-9b8a-5ce0baea773c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e0f991-19b6-499d-896b-1b3fb9f2e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "current_rdd = mini_batch\n",
    "\n",
    "# Collect data from the RDD\n",
    "tabular_data = current_rdd.map(lambda line: line[0].split('\\t'))\n",
    "\n",
    "\n",
    "# Convert the tabular data to RDD\n",
    "#rdd = spark_session.sparkContext.parallelize(tabular_data)\n",
    "\n",
    "# Print the first 3 rows\n",
    "# for row in tabular_data.take(3):\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c560eeb-bb10-4f8f-807e-d7f67929d615",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m header \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarketplace\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_parent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_title\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_category\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstar_rating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelpful_votes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_votes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvine\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverified_purchase\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_headline\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_body\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_votes_float\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhelpful_votes_float\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming spark_session is your SparkSession object and rdd is your RDD\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Convert RDD to DataFrame\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtabular_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----Schema----\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m--> 934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:605\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(schema):\n\u001b[0;32m--> 605\u001b[0m             \u001b[43mstruct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    606\u001b[0m             struct\u001b[38;5;241m.\u001b[39mnames[i] \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, StructType):\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "header = ['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category',\n",
    "          'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date', 'total_votes_float', 'helpful_votes_float']\n",
    "\n",
    "# Assuming spark_session is your SparkSession object and rdd is your RDD\n",
    "# Convert RDD to DataFrame\n",
    "df = spark_session.createDataFrame(tabular_data, schema=header)\n",
    "print(\"----Schema----\")\n",
    "df.printSchema()\n",
    "\n",
    "#add review length\n",
    "df = df.withColumn(\"review_length\", length(df[\"review_body\"]))\n",
    "# as it is string now\n",
    "df = df.withColumn(\"total_votes_float\", df[\"total_votes\"].cast(\"float\"))\n",
    "df = df.withColumn(\"helpful_votes_float\", df[\"helpful_votes\"].cast(\"float\"))\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = df.corr(\"review_length\", \"helpful_votes_float\")\n",
    "\n",
    "# Print the correlation\n",
    "print(\"Correlation between review length and helpful votes:\", correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e515d-f0a8-4b01-9a40-ff9df8545a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99195cf-4148-4ae4-94a9-68ad0e35a531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0443ce27-7363-4062-a92c-ebbe6d058d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
