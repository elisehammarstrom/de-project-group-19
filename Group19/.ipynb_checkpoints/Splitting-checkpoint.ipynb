{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad3bfcb-d64b-4a49-ba26-6a97234d0c29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 07:40:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://de-i-19:7077\")\\\n",
    "        .appName(\"Project_Group19\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"300s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", \"false\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "# spark_context.setLogLevel(\"INFO\")\n",
    "spark_context.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c15983e-72c0-46ad-be4b-44ae09cb608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports \n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf81cc41-d266-4e97-81a6-54b5d478cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lines(lines):\n",
    "    tokenized_lines = lines.map(lambda line: line.split('\"'))\n",
    "    return tokenized_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec09eb0b-a41a-4d22-8cd3-4e1a6fc0dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tabular_data(lines):\n",
    "    tabular_data = []\n",
    "\n",
    "    for line in lines :\n",
    "        csv_line = StringIO(line[0].replace('\\t', ','))\n",
    "        reader = csv.reader(csv_line, delimiter=',')\n",
    "        tabular_data.extend(reader)\n",
    "\n",
    "    # Convert the tabular data to RDD\n",
    "    rdd = spark_session.sparkContext.parallelize(tabular_data)\n",
    "\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cb6df7-ccb7-4e96-96ba-c78519f9d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(lines):\n",
    "    lowercased_lines = lines.map(lambda line: line.lower())\n",
    "    tokenized_lines = tokenize_lines(lowercased_lines)\n",
    "    return tokenized_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7751032-2169-4c8b-8395-8b699404a475",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marketplace\\tcustomer_id\\treview_id\\tproduct_id\\tproduct_parent\\tproduct_title\\tproduct_category\\tstar_rating\\thelpful_votes\\ttotal_votes\\tvine\\tverified_purchase\\treview_headline\\treview_body\\treview_date', 'US\\t9970739\\tR8EWA1OFT84NX\\tB00GSP5D94\\t329991347\\tSummer Infant SwaddleMe Adjustable Infant Wrap, 3 Count\\tBaby\\t5\\t0\\t0\\tN\\tY\\tGreat swaddled blankets\\tLoved these swaddle blankets and so did my daughter, she slept so much better when tightly swaddled. Only problem was we had a very hard time getting her to sleep without it when she started rolling.\\t2015-08-31', 'US\\t23538442\\tR2JWY4YRQD4FOP\\tB00YYDDZGU\\t646108902\\tPacifier Clip Girl (3 Pack) Ziggy Baby 2-Sided Design Pacifier Holder\\tBaby\\t5\\t0\\t0\\tN\\tN\\tToo cute and really nice\\tThese are adorable pacifier clips. SavvyBaby has elevated the Pacifier Holder to a new level. These are double sided strips of cotton with a loop on one end and a solid clasp on the other.<br /><br />They can be used for not only keep pacifiers off the floor but also for toys, bibs, teethers, and other small items. We looped two together to use as a bib holder while dining out..perfect with the cloth napkin from the restaurant.<br /><br />The pink and brown designs are just to pretty. They are perfect for little girls.<br /><br />I like that the pack includes three clips. It really makes this a great value purchase.<br /><br />Promotional or discounted product provided for review.\\t2015-08-31', 'US\\t8273344\\tRL5ESX231LZ0B\\tB00BUBNZC8\\t642922361\\tUdder Covers - Breast Feeding Nursing Cover\\tBaby\\t5\\t0\\t0\\tN\\tY\\tFive Stars\\tGreat gift\\t2015-08-31', \"US\\t24557753\\tRRMS9ZWJ2KD08\\tB00AWLZFTS\\t494272733\\tGerber Graduates Fun Pack Utensils\\tBaby\\t5\\t0\\t0\\tN\\tY\\tCute; wash up nicely in dishwasher.\\tThese forks are great for my 10 month old daughter! Wanted to find a nice plastic fork for her and I couldn't find a set without spoons but I really like the spoons as well. Nice size for small mouths.\\t2015-08-31\", 'US\\t46263340\\tR14I3ZG5E6S7YM\\tB00KM60D3Q\\t305813185\\tSummer Infant Ultra Sight Pan/Scan/Zoom Video Baby Monitor\\tBaby\\t5\\t0\\t0\\tN\\tY\\tLove it!\\tI wanted something for piece of mind with my little one, but I wanted to stay under $150. This monitor suits our needs, and the price was great.\\t2015-08-31', 'US\\t24557753\\tR13EPSFP5DODN5\\tB00PQMRZG4\\t607341708\\tSummer Infant Keep Me Clean Disposable Bibs, 20-Count\\tBaby\\t4\\t0\\t0\\tN\\tY\\tRips easily.\\tLoved with daughter about 4-6 months. After that, child will realize they are easily ripped off and the bib is useless.\\t2015-08-31', \"US\\t33520065\\tR6RBP4HTE67SY\\tB005DL5970\\t971881542\\tNatural HE Powder Laundry Detergent for Hard Water by Rockin' Green, Perfect for Cloth Diapers, Hard Rock Formula for Hard Water, Up to 90 Loads Per Bag, 45 oz.\\tBaby\\t5\\t0\\t0\\tN\\tY\\tCloth Diaper Detergent\\tThis is a great detergent for cloth diapers!!!\\t2015-08-31\", \"US\\t20241560\\tR15B3EU40RSU2W\\tB00C6D2WL4\\t93827401\\tDr. Brown's Bottle Brush, Pink\\tBaby\\t5\\t0\\t0\\tN\\tY\\tFive Stars\\tgood\\t2015-08-31\", \"US\\t9987983\\tRP4DD53A4ZJA2\\tB0083973FK\\t958629336\\tSposie Booster Pads Diaper Doubler, 90 Count, 3 Packs of 30 Pads\\tBaby\\t5\\t0\\t0\\tN\\tY\\tFive Stars\\tWith these my milk/water loving toddler doesn't wet himself at night.\\t2015-08-31\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark_context.textFile(\"hdfs://192.168.2.246:9000/user/ubuntu/amazon_reviews_us_Baby_v1_00.tsv\")\n",
    "print(data.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa1b2aa1-1382-437a-9b4e-26b92270e36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "tokenized_lines = preprocess_data(data)\n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aff8b8b-39d3-4c81-9324-244074247c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch = tokenized_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33d8f0b5-2838-4b9e-9b8a-5ce0baea773c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e0f991-19b6-499d-896b-1b3fb9f2e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "current_rdd = mini_batch\n",
    "\n",
    "# Collect data from the RDD\n",
    "tabular_data = current_rdd.map(lambda line: line[0].split('\\t'))\n",
    "\n",
    "\n",
    "# Convert the tabular data to RDD\n",
    "#rdd = spark_session.sparkContext.parallelize(tabular_data)\n",
    "\n",
    "# Print the first 3 rows\n",
    "# for row in tabular_data.take(3):\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c560eeb-bb10-4f8f-807e-d7f67929d615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Schema----\n",
      "root\n",
      " |-- marketplace: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_parent: string (nullable = true)\n",
      " |-- product_title: string (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- star_rating: string (nullable = true)\n",
      " |-- helpful_votes: string (nullable = true)\n",
      " |-- total_votes: string (nullable = true)\n",
      " |-- vine: string (nullable = true)\n",
      " |-- verified_purchase: string (nullable = true)\n",
      " |-- review_headline: string (nullable = true)\n",
      " |-- review_body: string (nullable = true)\n",
      " |-- review_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+-------------+-----------------+-------------------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|review_length|total_votes_float|helpful_votes_float|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+-------------+-----------------+-------------------+\n",
      "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|           11|             null|               null|\n",
      "|         us|    9970739| r8ewa1oft84nx|b00gsp5d94|     329991347|summer infant swa...|            baby|          5|            0|          0|   n|                y|great swaddled bl...|loved these swadd...| 2015-08-31|          199|              0.0|                0.0|\n",
      "|         us|   23538442|r2jwy4yrqd4fop|b00yyddzgu|     646108902|pacifier clip gir...|            baby|          5|            0|          0|   n|                n|too cute and real...|these are adorabl...| 2015-08-31|          689|              0.0|                0.0|\n",
      "|         us|    8273344| rl5esx231lz0b|b00bubnzc8|     642922361|udder covers - br...|            baby|          5|            0|          0|   n|                y|          five stars|          great gift| 2015-08-31|           10|              0.0|                0.0|\n",
      "|         us|   24557753| rrms9zwj2kd08|b00awlzfts|     494272733|gerber graduates ...|            baby|          5|            0|          0|   n|                y|cute; wash up nic...|these forks are g...| 2015-08-31|          201|              0.0|                0.0|\n",
      "|         us|   46263340|r14i3zg5e6s7ym|b00km60d3q|     305813185|summer infant ult...|            baby|          5|            0|          0|   n|                y|            love it!|i wanted somethin...| 2015-08-31|          144|              0.0|                0.0|\n",
      "|         us|   24557753|r13epsfp5dodn5|b00pqmrzg4|     607341708|summer infant kee...|            baby|          4|            0|          0|   n|                y|        rips easily.|loved with daught...| 2015-08-31|          119|              0.0|                0.0|\n",
      "|         us|   33520065| r6rbp4hte67sy|b005dl5970|     971881542|natural he powder...|            baby|          5|            0|          0|   n|                y|cloth diaper dete...|this is a great d...| 2015-08-31|           46|              0.0|                0.0|\n",
      "|         us|   20241560|r15b3eu40rsu2w|b00c6d2wl4|      93827401|dr. brown's bottl...|            baby|          5|            0|          0|   n|                y|          five stars|                good| 2015-08-31|            4|              0.0|                0.0|\n",
      "|         us|    9987983| rp4dd53a4zja2|b0083973fk|     958629336|sposie booster pa...|            baby|          5|            0|          0|   n|                y|          five stars|with these my mil...| 2015-08-31|           69|              0.0|                0.0|\n",
      "|         us|   52570308|r2c99djeo4rz4k|b00rlyg2s2|     147324304|abiie beyond wood...|            baby|          5|            3|          4|   n|                y|so far i love thi...|so far i love thi...| 2015-08-31|         1042|              4.0|                3.0|\n",
      "|         us|    9287389| rev51ew323h8w|b010ux9t5i|     446691106|lovinglove baby g...|            baby|          5|            0|          0|   n|                y|          five stars|           love them| 2015-08-31|            9|              0.0|                0.0|\n",
      "|         us|   32840762|r2gq3w03wiuzke|b00vwby7sc|     271204734|bugzi stroller ho...|            baby|          5|            0|          0|   n|                y|love these hooks ...|love these hooks ...| 2015-08-31|          184|              0.0|                0.0|\n",
      "|         us|    7797182| rti1yi7k6ge3d|b006zbph24|      67911244|born free 5 oz. b...|            baby|          5|            0|          0|   n|                y|          five stars|           very good| 2015-08-31|            9|              0.0|                0.0|\n",
      "|         us|   14788115|r3v9c2c0spszu6|b00ugv8beu|     613360092|baby bandana bibs...|            baby|          5|            0|          0|   n|                y|            perfect!|love these bibs! ...| 2015-08-31|           52|              0.0|                0.0|\n",
      "|         us|   37909065|r1lb42xcssclv6|b005biooyo|     527977399|flip stay-dry ins...|            baby|          5|            0|          0|   n|                y|best inserts for ...|i love these. the...| 2015-08-31|          128|              0.0|                0.0|\n",
      "|         us|   15935520|r113nwcw6sttmc|b0071d1aki|     634188771|aqueduck the orig...|            baby|          5|            0|          0|   n|                y|        very helpful|this worked exact...| 2015-08-31|          171|              0.0|                0.0|\n",
      "|         us|   16308044| rwrn5xk337n41|b00m2f0oys|     166133791|motorola baby mon...|            baby|          1|            0|          0|   n|                n|very dissapointed...|i am so disappoin...| 2015-08-31|          794|              0.0|                0.0|\n",
      "|         us|    8168178| rf4wl3qep3pvi|b00qcbd5as|     294351494|minnie mouse delu...|            baby|          1|            0|          0|   n|                y|            one star|ordered minnie mo...| 2015-08-31|           82|              0.0|                0.0|\n",
      "|         us|   23299101|r2drl5nrodvq3z|b00sn6f9ng|       3470998|rhoost nail clipp...|            baby|          5|            2|          2|   n|                y|if fits so comfor...|this is an absolu...| 2015-08-31|          455|              2.0|                2.0|\n",
      "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+-------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 07:40:21 WARN TaskSetManager: Lost task 1.0 in stage 3.0 (TID 4) (192.168.2.246 executor 0): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 15 fields are required while 6 values are provided.\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:754)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foreach(WholeStageCodegenExec.scala:758)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foldLeft(WholeStageCodegenExec.scala:758)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.aggregate(WholeStageCodegenExec.scala:758)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/04 07:40:21 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3) (192.168.2.246 executor 0): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 15 fields are required while 13 values are provided.\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:754)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foreach(WholeStageCodegenExec.scala:758)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foldLeft(WholeStageCodegenExec.scala:758)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.aggregate(WholeStageCodegenExec.scala:758)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/04 07:40:21 WARN TaskSetManager: Lost task 2.0 in stage 3.0 (TID 5) (192.168.2.246 executor 0): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 15 fields are required while 14 values are provided.\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:754)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foreach(WholeStageCodegenExec.scala:758)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foldLeft(WholeStageCodegenExec.scala:758)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.aggregate(WholeStageCodegenExec.scala:758)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/04 07:40:22 ERROR TaskSetManager: Task 3 in stage 3.0 failed 4 times; aborting job\n",
      "24/03/04 07:40:22 WARN TaskSetManager: Lost task 2.3 in stage 3.0 (TID 15) (192.168.2.246 executor 0): TaskKilled (Stage cancelled)\n",
      "24/03/04 07:40:22 WARN TaskSetManager: Lost task 0.3 in stage 3.0 (TID 14) (192.168.2.246 executor 0): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o96.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 3.0 failed 4 times, most recent failure: Lost task 3.3 in stage 3.0 (TID 13) (192.168.2.246 executor 0): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 15 fields are required while 13 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:754)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foreach(WholeStageCodegenExec.scala:758)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foldLeft(WholeStageCodegenExec.scala:758)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.aggregate(WholeStageCodegenExec.scala:758)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.collectStatisticalData(StatFunctions.scala:168)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.pearsonCorrelation(StatFunctions.scala:115)\n\tat org.apache.spark.sql.DataFrameStatFunctions.corr(DataFrameStatFunctions.scala:159)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 15 fields are required while 13 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:754)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foreach(WholeStageCodegenExec.scala:758)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foldLeft(WholeStageCodegenExec.scala:758)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.aggregate(WholeStageCodegenExec.scala:758)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Calculate correlation\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m correlation \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreview_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhelpful_votes_float\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Print the correlation\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrelation between review length and helpful votes:\u001b[39m\u001b[38;5;124m\"\u001b[39m, correlation)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:2883\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[0;34m(self, col1, col2, method)\u001b[0m\n\u001b[1;32m   2878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2879\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently only the calculation of the Pearson Correlation \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoefficient is supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m     )\n\u001b[0;32m-> 2883\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o96.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 3.0 failed 4 times, most recent failure: Lost task 3.3 in stage 3.0 (TID 13) (192.168.2.246 executor 0): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 15 fields are required while 13 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:754)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foreach(WholeStageCodegenExec.scala:758)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foldLeft(WholeStageCodegenExec.scala:758)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.aggregate(WholeStageCodegenExec.scala:758)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.collectStatisticalData(StatFunctions.scala:168)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.pearsonCorrelation(StatFunctions.scala:115)\n\tat org.apache.spark.sql.DataFrameStatFunctions.corr(DataFrameStatFunctions.scala:159)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 15 fields are required while 13 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:754)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foreach(WholeStageCodegenExec.scala:758)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.foldLeft(WholeStageCodegenExec.scala:758)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.aggregate(WholeStageCodegenExec.scala:758)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "from pyspark.sql.functions import length\n",
    "\n",
    "header = ['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category',\n",
    "          'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date']\n",
    "\n",
    "# Assuming spark_session is your SparkSession object and rdd is your RDD\n",
    "# Convert RDD to DataFrame\n",
    "df = spark_session.createDataFrame(tabular_data, schema=header)\n",
    "print(\"----Schema----\")\n",
    "df.printSchema()\n",
    "\n",
    "#add review length\n",
    "df = df.withColumn(\"review_length\", length(df[\"review_body\"]))\n",
    "# as it is string now\n",
    "df = df.withColumn(\"total_votes_float\", df[\"total_votes\"].cast(\"float\"))\n",
    "df = df.withColumn(\"helpful_votes_float\", df[\"helpful_votes\"].cast(\"float\"))\n",
    "df.show()\n",
    "\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = df.corr(\"review_length\", \"helpful_votes_float\")\n",
    "\n",
    "# Print the correlation\n",
    "print(\"Correlation between review length and helpful votes:\", correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e515d-f0a8-4b01-9a40-ff9df8545a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99195cf-4148-4ae4-94a9-68ad0e35a531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0443ce27-7363-4062-a92c-ebbe6d058d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
