{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad3bfcb-d64b-4a49-ba26-6a97234d0c29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/05 13:56:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.functions import corr, length, col, when\n",
    "import time\n",
    "\n",
    "#change spark.cores.max to change the total number of cores used. Keep executor cores set to 1 since the small vms have only one core having more here causes a ressource issue\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://de-i-19:7077\")\\\n",
    "        .appName(\"Project_Group19\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"300s\")\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .config(\"spark.executor.cores\",1)\\\n",
    "        .config(\"spark.cores.max\",4)\\\n",
    "        .config(\"spark.executor.memory\",\"1g\")\\\n",
    "        .config(\"spark.shuffle.service.enabled\", \"false\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "# spark_context.setLogLevel(\"INFO\")\n",
    "spark_context.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864b34e-de5e-41b9-b1d0-ebc77b1e2883",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a319d877-2d4b-40a7-a3a8-28c0ab32eb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'mapPartitions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mgetsizeof(obj)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate the size of each partition\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m partition_sizes \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m(\u001b[38;5;28;01mlambda\u001b[39;00m partition: [\u001b[38;5;28msum\u001b[39m(get_object_size(record) \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m partition)])\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate the total size of the RDD in bytes\u001b[39;00m\n\u001b[1;32m     10\u001b[0m total_size_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(partition_sizes)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1988\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1979\u001b[0m \n\u001b[1;32m   1980\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 1988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   1990\u001b[0m     )\n\u001b[1;32m   1991\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'mapPartitions'"
     ]
    }
   ],
   "source": [
    "\n",
    "data = spark_session.read.csv(\"hdfs://192.168.2.246:9000/user/ubuntu/amazon_reviews_us_Baby_v1_00.tsv\", sep=\"\\t\", header=True, inferSchema=True)\n",
    "def get_object_size(obj):\n",
    "    import sys\n",
    "    return sys.getsizeof(obj)\n",
    "\n",
    "# Calculate the size of each partition\n",
    "partition_sizes = data.mapPartitions(lambda partition: [sum(get_object_size(record) for record in partition)]).collect()\n",
    "\n",
    "# Calculate the total size of the RDD in bytes\n",
    "total_size_bytes = sum(partition_sizes)\n",
    "\n",
    "# Convert the size to gigabytes\n",
    "total_size_gb = total_size_bytes / (1024.0 ** 3)  # Convert bytes to GB\n",
    "\n",
    "print(\"Size of the RDD:\", total_size_gb, \"GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27ed18-3662-499f-921a-ae89264c065e",
   "metadata": {},
   "source": [
    "# Start Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1929c2-ddb1-4ba4-9be1-a46a8fe701b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e25146-06e6-4887-aa78-4eb3e10e4101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add the derived columns\n",
    "\n",
    "data = data.withColumn(\"vine_numeric\", when(col(\"vine\") == \"Y\", 1).otherwise(0))\n",
    "data = data.withColumn(\"verified_purchase_numeric\", when(col(\"verified_purchase\") == \"Y\", 1).otherwise(0))\n",
    "data = data.withColumn(\"review_length\", length(col(\"review_body\")))\n",
    "data = data.withColumn(\"total_votes_float\", col(\"total_votes\").cast(\"float\"))\n",
    "data = data.withColumn(\"helpful_votes_float\", col(\"helpful_votes\").cast(\"float\"))\n",
    "\n",
    "#data.printSchema()\n",
    "#data.show(5)\n",
    "\n",
    "# Calculate correlation\n",
    "correlation_helpful = data.corr(\"review_length\", \"helpful_votes_float\")\n",
    "correlation_vine = data.corr(\"review_length\", \"vine_numeric\")\n",
    "correlation_star_rating = data.corr(\"review_length\", \"star_rating\")\n",
    "correlation_verified_purchase = data.corr(\"review_length\", \"verified_purchase_numeric\")\n",
    "\n",
    "print(\"Correlation between review length and helpful votes:\", correlation_helpful)\n",
    "print(\"Correlation between review length and vine:\", correlation_vine)\n",
    "print(\"Correlation between review length and star rating:\", correlation_star_rating)\n",
    "print(\"Correlation between review length and verified purchase:\", correlation_verified_purchase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c55994-0f33-401b-890f-4813dd153cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(\"Experiment took\", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b7876-392c-4442-b6fc-92f42cc41078",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
